{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text classification using bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYjTeOYHntkt"
      },
      "outputs": [],
      "source": [
        "!git clone --depth 1 -b v2.3.0 https://github.com/tensorflow/models.git\n",
        "!pip install -Uqr models/official/requirements.txt "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import sys\n",
        "sys.path.append('models')\n",
        "from official.nlp.data import classifier_data_lib\n",
        "from official.nlp.bert import tokenization\n",
        "from official.nlp import optimization\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "gaZCNE3ro9OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://archive.org/download/fine-tune-bert-tensorflow-train.csv/train.csv.zip',\n",
        "                 compression='zip', low_memory=False)\n",
        "\n",
        "\n",
        "\n",
        "train_df, remaining = train_test_split(df, random_state=42, train_size=0.0075, stratify=df.target.values)\n",
        "valid_df, _ = train_test_split(remaining, random_state=42, train_size=0.00075, stratify=remaining.target.values)\n",
        "train_df.shape, valid_df.shape\n",
        "\n",
        "\n",
        "with tf.device('/cpu:0'):\n",
        "  train_data = tf.data.Dataset.from_tensor_slices((train_df.question_text.values, train_df.target.values))\n",
        "  valid_data = tf.data.Dataset.from_tensor_slices((valid_df.question_text.values, valid_df.target.values))\n",
        "\n",
        "\n",
        "label_list = [0, 1]\n",
        "max_seq_length = 128\n",
        "train_batch_size = 32\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\",\n",
        "                            trainable=True)\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
      ],
      "metadata": {
        "id": "zbm82B7EpMII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_feature(text, label, label_list=label_list, max_seq_length=max_seq_length, tokenizer=tokenizer):\n",
        "  example = classifier_data_lib.InputExample(guid = None,\n",
        "                                            text_a = text.numpy(), \n",
        "                                            text_b = None, \n",
        "                                            label = label.numpy())\n",
        "  feature = classifier_data_lib.convert_single_example(0, example, label_list,\n",
        "                                    max_seq_length, tokenizer)\n",
        "  \n",
        "  return (feature.input_ids, feature.input_mask, feature.segment_ids, feature.label_id)"
      ],
      "metadata": {
        "id": "Nw8UGqt8qOfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_feature_map(text, label):\n",
        "  input_ids, input_mask, segment_ids, label_id = tf.py_function(to_feature, inp=[text, label], \n",
        "                                Tout=[tf.int32, tf.int32, tf.int32, tf.int32])\n",
        "\n",
        "  # py_func doesn't set the shape of the returned tensors.\n",
        "  input_ids.set_shape([max_seq_length])\n",
        "  input_mask.set_shape([max_seq_length])\n",
        "  segment_ids.set_shape([max_seq_length])\n",
        "  label_id.set_shape([])\n",
        "\n",
        "  x = {\n",
        "        'input_word_ids': input_ids,\n",
        "        'input_mask': input_mask,\n",
        "        'input_type_ids': segment_ids\n",
        "    }\n",
        "  return (x, label_id)"
      ],
      "metadata": {
        "id": "aj1jiTVfqb5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/cpu:0'):\n",
        "  # train\n",
        "  train_data = (train_data.map(to_feature_map,\n",
        "                              num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "                          #.cache()\n",
        "                          .shuffle(1000)\n",
        "                          .batch(32, drop_remainder=True)\n",
        "                          .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "  # valid\n",
        "  valid_data = (valid_data.map(to_feature_map,\n",
        "                            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "                          .batch(32, drop_remainder=True)\n",
        "                          .prefetch(tf.data.experimental.AUTOTUNE))"
      ],
      "metadata": {
        "id": "wpObFcBKqh-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "  input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                      name=\"input_word_ids\")\n",
        "  input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                  name=\"input_mask\")\n",
        "  input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n",
        "                                  name=\"input_type_ids\")\n",
        "\n",
        "  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\n",
        "\n",
        "  drop = tf.keras.layers.Dropout(0.4)(pooled_output)\n",
        "  output = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"output\")(drop)\n",
        "\n",
        "  model = tf.keras.Model(\n",
        "    inputs={\n",
        "        'input_word_ids': input_word_ids,\n",
        "        'input_mask': input_mask,\n",
        "        'input_type_ids': input_type_ids\n",
        "    },\n",
        "    outputs=output)\n",
        "  return model"
      ],
      "metadata": {
        "id": "8pJHZljEqsp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "model.summary()\n",
        "\n",
        "epochs = 4\n",
        "history = model.fit(train_data,\n",
        "                    validation_data=valid_data,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1)"
      ],
      "metadata": {
        "id": "tk03HPMUq17I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_example = [\"ayaan like to beat weak people\",\\\n",
        "                  \"we should kill the each other \",\\\n",
        "                  \"Raping is act of charity \",\\\n",
        "                  \" we should tease each other \",\\\n",
        "                  \"the person who is watching this notebook can see give an input less than 32 tokens to get a prediction, that if its good or bad\",\\\n",
        "                  \"TYPE HERE, INSIDE THE QUOTATION \"]\n",
        "test_data = tf.data.Dataset.from_tensor_slices((sample_example, [0]*len(sample_example)))\n",
        "test_data = (test_data.map(to_feature_map).batch(1))\n",
        "preds = model.predict(test_data)\n",
        "['Toxic' if pred >=0.5 else 'Sincere' for pred in preds]"
      ],
      "metadata": {
        "id": "z5e0ziw-rma0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}